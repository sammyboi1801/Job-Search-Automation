# ğŸš€ Job Search Automation CLI Tool

A production-ready, terminal-based job alert engine. It scrapes multiple job boards, deduplicates listings, scores them for relevance, and emails you only the **new** ones â€” on a configurable schedule.

---

## ğŸ“ Project Structure

```
job_automation/
â”œâ”€â”€ main.py                    â† CLI entry point
â”œâ”€â”€ config.yaml                â† All user configuration
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scheduler.py               â† Orchestrates runs + APScheduler loop
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ base_scraper.py        â† Abstract base + Job model + rate limiting
â”‚   â”œâ”€â”€ indeed_scraper.py
â”‚   â”œâ”€â”€ linkedin_scraper.py    â† Uses Playwright (headless Chromium)
â”‚   â”œâ”€â”€ simplify_scraper.py
â”‚   â”œâ”€â”€ handshake_scraper.py
â”‚   â””â”€â”€ google_jobs_scraper.py â† SerpAPI or HTML fallback
â”œâ”€â”€ notifier/
â”‚   â”œâ”€â”€ email_notifier.py      â† HTML email digest via SMTP
â”‚   â””â”€â”€ telegram_notifier.py   â† Optional Telegram push
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ database.py            â† SQLite (dedup + keyword management)
â”œâ”€â”€ logs/                      â† Auto-created. Rotating log files.
â””â”€â”€ exports/                   â† Auto-created. CSV exports.
```

---

## âš¡ Quick Start

### 1. Clone & install dependencies

```bash
git clone https://github.com/yourname/job-automation
cd job-automation
python -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Install Playwright browser

```bash
playwright install chromium
```

### 3. Configure

Edit `config.yaml`:

```yaml
search:
  keywords:
    - "Software Engineer"
    - "Machine Learning Engineer"
  locations:
    - "San Francisco, CA"
    - "Remote"

email:
  recipients:
    - "you@gmail.com"

scrapers:
  enabled:
    - indeed
    - linkedin
    - simplify
```

### 4. Set environment variables

```bash
# Required for email
export EMAIL_USER="youremail@gmail.com"
export EMAIL_PASS="your-gmail-app-password"   # NOT your real password

# Optional: LinkedIn login (more results)
export LINKEDIN_EMAIL="you@email.com"
export LINKEDIN_PASS="yourpassword"

# Optional: SerpAPI for Google Jobs
export SERPAPI_KEY="your-key-here"

# Optional: Telegram
export TELEGRAM_TOKEN="123456:ABC..."
export TELEGRAM_CHAT_ID="-100123456789"
```

> **Gmail App Password**: Go to [Google Account â†’ Security â†’ 2-Step Verification â†’ App passwords](https://myaccount.google.com/apppasswords). Generate a password for "Mail".

### 5. Test email

```bash
python main.py --test-email
```

### 6. Run once to verify everything works

```bash
python main.py --dry-run      # no email sent
python main.py --run-now      # sends email with new jobs
```

### 7. Start the scheduler (runs every 3 hours by default)

```bash
python main.py
```

---

## ğŸ–¥ï¸ CLI Reference

| Command | Description |
|---|---|
| `python main.py` | Start the scheduler (default: every 3h) |
| `python main.py --run-now` | Run once immediately, send email |
| `python main.py --dry-run` | Run once, save to DB but skip notifications |
| `python main.py --test-email` | Send a test email |
| `python main.py --add-keyword "ML Engineer"` | Add keyword to DB |
| `python main.py --remove-keyword "Data Analyst"` | Remove keyword from DB |
| `python main.py --list-config` | Print all active settings |
| `python main.py --list-jobs 30` | Show 30 most recent unseen jobs |
| `python main.py --export-csv` | Export all unseen jobs to CSV |

---

## âš™ï¸ Configuration Reference (`config.yaml`)

```yaml
search:
  keywords: [...]         # Job titles / roles to search
  tags: [...]             # Used for relevance scoring (AI, ML, Pythonâ€¦)
  locations: [...]        # List of cities or "Remote"
  experience_level: ""    # entry | mid | senior | lead (leave blank = any)
  remote_filter: false    # true = remote listings only

scheduler:
  interval_hours: 3       # Run frequency
  send_empty_email: false # Send email even when no new jobs

scrapers:
  enabled:                # Which scrapers to run
    - indeed
    - linkedin
    - simplify
    - handshake
    - google_jobs
  rate_limit_seconds: 3   # Delay between requests
  max_retries: 3
  headless: true          # Playwright headless mode

email:
  recipients: ["you@email.com"]
  smtp_host: "smtp.gmail.com"
  smtp_port: 587

telegram:
  enabled: false          # Set to true + env vars to enable

export:
  csv_enabled: true
  csv_path: "exports/jobs_export.csv"
```

---

## ğŸ” Anti-Bot & Ethical Scraping

| Platform | Method | Notes |
|---|---|---|
| Indeed | `cloudscraper` + rotating UA | Respects rate limits |
| LinkedIn | Playwright headless Chromium | Random delays, UA spoofing |
| Simplify | requests + JSON API | Lightweight, official-ish endpoint |
| Handshake | requests JSON API â†’ Playwright fallback | |
| Google Jobs | SerpAPI (preferred) or HTML fallback | |

- All scrapers implement **exponential back-off** on failures
- `robots.txt` is checked before fetching
- One site failing **never** crashes others

---

## ğŸ”„ Running as a Background Process

### macOS / Linux â€” `nohup`

```bash
nohup python main.py > logs/stdout.log 2>&1 &
echo $! > .pid
# Stop: kill $(cat .pid)
```

### macOS / Linux â€” `screen`

```bash
screen -S job-bot
source .venv/bin/activate
python main.py
# Detach: Ctrl+A then D
# Reattach: screen -r job-bot
```

### Linux â€” `systemd` service

Create `/etc/systemd/system/job-automation.service`:

```ini
[Unit]
Description=Job Search Automation
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/home/ubuntu/job_automation
ExecStart=/home/ubuntu/job_automation/.venv/bin/python main.py
Environment="EMAIL_USER=you@gmail.com"
Environment="EMAIL_PASS=yourapppassword"
Restart=on-failure
RestartSec=30

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl daemon-reload
sudo systemctl enable job-automation
sudo systemctl start job-automation
sudo systemctl status job-automation
```

### Windows â€” Task Scheduler

1. Open Task Scheduler â†’ Create Basic Task
2. Set trigger: **Daily**, repeat every 3 hours
3. Action: Start program â†’ `python` with argument `C:\path\to\job_automation\main.py --run-now`

---

## â˜ï¸ AWS EC2 Deployment (Bonus)

```bash
# 1. Spin up a t3.micro (free tier) running Ubuntu 22.04
# 2. SSH in and install dependencies
sudo apt update && sudo apt install -y python3-pip python3-venv git
git clone https://github.com/yourname/job-automation
cd job-automation
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
playwright install chromium
playwright install-deps chromium   # install system deps

# 3. Set env vars (use a .env file or AWS Secrets Manager)
cp .env.example .env
nano .env  # fill in EMAIL_USER, EMAIL_PASS, etc.
source .env

# 4. Run as systemd service (see above) OR use tmux
tmux new -s jobs
python main.py
# Ctrl+B, D to detach
```

---

## ğŸ§© Adding a New Job Board

1. Create `scrapers/mynewsite_scraper.py`
2. Inherit from `BaseScraper`, set `SOURCE_NAME = "mynewsite"`
3. Implement `search(self, keyword: str, location: str) -> List[Job]`
4. Register it in `scheduler.py`:
   ```python
   from scrapers.mynewsite_scraper import MyNewSiteScraper
   SCRAPER_REGISTRY["mynewsite"] = MyNewSiteScraper
   ```
5. Add `"mynewsite"` to `scrapers.enabled` in `config.yaml`

That's it. The dedup, scoring, notification and scheduling all happen automatically.

**Minimal scraper template:**

```python
from scrapers.base_scraper import BaseScraper, Job
from typing import List

class MyNewSiteScraper(BaseScraper):
    SOURCE_NAME = "mynewsite"

    def search(self, keyword: str, location: str) -> List[Job]:
        jobs = []
        resp = self._get(f"https://mynewsite.com/jobs?q={keyword}&l={location}")
        if not resp:
            return jobs
        # parse resp.text / resp.json() ...
        jobs.append(Job(title="...", company="...", url="..."))
        return jobs
```

---

## ğŸ§ª Testing & Debug

```bash
# Dry run (no email, but logs everything)
python main.py --dry-run

# Verbose logging
# In config.yaml: logging.level: DEBUG

# Show queued jobs in DB
python main.py --list-jobs 50

# Tail logs
tail -f logs/job_scraper.log
```

---

## ğŸ† Feature Summary

| Feature | Status |
|---|---|
| Multi-platform scraping (5 sites) | âœ… |
| Keyword + location Ã— scraper matrix | âœ… |
| SQLite deduplication | âœ… |
| Relevance scoring 0â€“100 | âœ… |
| HTML email digest | âœ… |
| Telegram notifications | âœ… |
| CSV export | âœ… |
| APScheduler every X hours | âœ… |
| Playwright anti-bot for LinkedIn | âœ… |
| Rotating user agents | âœ… |
| robots.txt compliance | âœ… |
| Graceful failure isolation | âœ… |
| CLI keyword management | âœ… |
| systemd / background process docs | âœ… |
| AWS EC2 deployment guide | âœ… |
