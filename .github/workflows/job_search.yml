name: Job Search Automation

on:
  schedule:
    # Runs every 4 hours (adjust as needed)
    - cron: '0 */4 * * *'
  workflow_dispatch: 
    # Allows you to click a button to run it manually

permissions:
  contents: write  # Critical: allows the action to commit the DB back to the repo

jobs:
  scrape-and-notify:
    runs-on: ubuntu-latest
    
    steps:
      # 1. Check out the repository code
      - name: Checkout code
        uses: actions/checkout@v4

      # 2. Set up Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip' # Caches dependencies to speed up runs

      # 3. Install Libraries & Browser
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps

      # 4. Run the Scraper
      - name: Run Job Scraper
        env:
          EMAIL_USER: ${{ secrets.EMAIL_USER }}
          EMAIL_PASS: ${{ secrets.EMAIL_PASS }}
          LINKEDIN_EMAIL: ${{ secrets.LINKEDIN_EMAIL }}
          LINKEDIN_PASS: ${{ secrets.LINKEDIN_PASS }}
          SERPAPI_KEY: ${{ secrets.SERPAPI_KEY }}
          TELEGRAM_TOKEN: ${{ secrets.TELEGRAM_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          # The --run-now flag forces it to run immediately
          python main.py --run-now

      # 5. Commit the updated database back to the repo
      # This ensures we remember which jobs we've already seen
      - name: Commit and Push DB changes
        run: |
          git config --global user.name "GitHub Action Job Bot"
          git config --global user.email "actions@github.com"
          
          # Add the database file specifically
          git add storage/jobs.db
          
          # Commit only if there are changes (prevent error if no new jobs found)
          git commit -m "Update job database [skip ci]" || exit 0
          
          # Push changes back to main branch
          git push
